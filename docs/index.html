<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Scan2Cap: Context-aware Dense Captioning in RGB-D Scans</title>
    <link rel="stylesheet" href="w3.css">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
</head>

<body>

<br/>
<br/>

<div class="w3-container" id="paper">
    <div class="w3-content" style="max-width:850px">
  
    <h2 align="center" id="title"><b>Scan2Cap: Context-aware Dense Captioning in RGB-D Scans</b></h2>
    <br/>

    <p align="center" class="center_text" id="authors">
        <a target="_blank" href="https://daveredrum.github.io/">Dave Zhenyu Chen</a><sup>1</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;

        <a target="_blank" href="https://aligholami.github.io/">Ali Gholami</a><sup>2</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;
        
        <a target="_blank" href="https://www.niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nie&szlig;ner</a><sup>1</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;

        <a target="_blank" href="https://angelxuanchang.github.io/">Angel X. Chang</a><sup>2</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;
    </p>

    <p class="center_text" align="center">
        <sup>1</sup>Technical University of Munich
        &nbsp; &nbsp; &nbsp;
        <sup>2</sup>Simon Fraser University
    </p>

    <br><center><img src="teaser.jpg" style="max-width:100%" /></center><br>

    <h3 class="w3-left-align" id="video"><b>Introduction</b></h3>
    <p>
        We introduce the task of dense captioning in 3D scans from commodity RGB-D sensors. As input, we assume a point cloud of a 3D scene; the expected output is the bounding boxes along with the descriptions for the underlying objects. To address the 3D object detection and description problems, we propose Scan2Cap, an end-to-end trained method, to detect objects in the input scene and describe them in natural language. We use an attention mechanism that generates descriptive tokens while referring to the related components in the local context. To reflect object relations (i.e. relative spatial relations) in the generated captions, we use a message passing graph module to facilitate learning object relation features. Our method can effectively localize and describe 3D objects in scenes from the ScanRefer dataset, outperforming 2D baseline methods by a significant margin (27.61% CiDEr@0.5IoUimprovement).
    </p>

    <h3 class="w3-left-align" id="video"><b>Video</b></h3>
    <p>
    <iframe width="850" height="480" src="https://www.youtube.com/embed/AgmIpDbwTCY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <p/>

    <h3 class="w3-left-align" id="video"><b>Results</b></h3>
    <p>
    <center><img src="qualitative.jpg" style="max-width:100%" /></center>
    Qualitative results from baseline methods and <span style="color: #61bcde; font-weight: bold;">Scan2Cap</span> with inaccurate parts of the generated caption underscored. Our method (<span style="color: #61bcde; font-weight: bold;">Scan2Cap</span>) produces good bounding boxes with descriptions for the target appearance and their relational interactions withobjects nearby.
    <p/>

    <h3 class="w3-left-align" id="publication"><b>Publication</b></h3>
    <a href="Scan2Cap.pdf" target="__blank">Paper</a> | <a href="https://arxiv.org/abs/2012.02206" target="__blank">arXiv</a> | <a href="https://github.com/daveredrum/Scan2Cap" target="__blank">Code</a>
    <center>
        <a href="Scan2Cap.pdf" target="__blank"><img src="paper.jpg" style="max-width:100%" /></a>
    </center><br>

    If you find our project useful, please consider citing us:
    <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 11px">

@misc{chen2020scan2cap,
    title={Scan2Cap: Context-aware Dense Captioning in RGB-D Scans}, 
    author={Dave Zhenyu Chen and Ali Gholami and Matthias Nie√üner and Angel X. Chang},
    year={2020},
    eprint={2012.02206},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

</pre>

    </div>


</div>

<br/>
<br/>

</body>
</html>